
<!DOCTYPE html>
<html lang="zh-CN" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>0x03 神经网络的学习 - 如是</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Fechin,"> 
    <meta name="description" content="学习的含义
​    神经网络的特征是从数据中进行学习。所谓从数据中进行学习，是指可以由数据自动决定权重参数的值。

损失函数
​    神经网络的学习通过某个指标表示现在的状态（神经网络的性能），,"> 
    <meta name="author" content="如是"> 
    <link rel="alternative" href="atom.xml" title="如是" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.jpg"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
<link rel="stylesheet" href="/css/diaspora.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">如是</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://example.com"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">0x03 神经网络的学习</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">0x03 神经网络的学习</h1>
        <div class="stuff">
            <span>四月 05, 2021</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/DL/" rel="tag">DL</a></li></ul>


        </div>
        <div class="content markdown">
            <h2 id="学习的含义"><a href="#学习的含义" class="headerlink" title="学习的含义"></a>学习的含义</h2><blockquote>
<p>​    神经网络的特征是从<em>数据中进行学习</em>。所谓从<em>数据中进行学习</em>，是指可以由数据自动决定<strong>权重参数的值</strong>。<span id="more"></span></p>
</blockquote>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><blockquote>
<p>​    神经网络的学习通过某个<strong>指标</strong>表示现在的状态（神经网络的性能），然后，以这个指标为基准寻找<strong>最优权重参数</strong>。所用的指标即被称为<strong>损失函数</strong>。损失函数一般为均方误差和交叉熵误差。</p>
</blockquote>
<h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><p>$$<br>E = \frac{1}{2}\displaystyle\sum_{k}(y_k-t_k)^2<br>$$</p>
<p>其中k表示数据的维数（即数据的个数），y<del>k</del>表示神经网络的输出，t<del>k</del>表示监督数据（即标签）。</p>
<blockquote>
<p>这里介绍一下<code>one-hot</code>编码，<code>one-hot</code>编码，是将正确解标签表示为1，其他标签表示为0的方法。</p>
</blockquote>
<p>均方误差表示的是神经网络的输出与正确解监督数据的各个元素之差的平方，再求和。</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_squared_error</span>(<span class="params">y, t</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.<span class="built_in">sum</span>( (y-t)**<span class="number">2</span> )</span><br></pre></td></tr></table></figure>





<h3 id="交叉熵误差-cross-entropy"><a href="#交叉熵误差-cross-entropy" class="headerlink" title="交叉熵误差  cross entropy"></a>交叉熵误差  <code>cross entropy</code></h3><p>交叉熵是用来比较两个概率分布的。它会告诉我们两个分布的相似程度。</p>
<p>在同一组结果上定义的两个概率分布p和q之间的交叉熵是由下列公式给出的：<br>$$<br>H(p,q) = -\displaystyle\sum_xp(x)\log q(x)<br>$$</p>
<p>$$<br>E = -\displaystyle\sum_{k}t_klog(y_k)<br>$$</p>
<p>交叉熵误差实际计算的是<em>正确解标签</em>与<em>神经网络在正确解上的输出的对数</em>的乘积取负数。在标签为<code>one-hot</code>编码的情况下实际计算的是<em>神经网络输出正确解概率的对数然后取负数</em>。因为在此，正确解的概率为1 ，即p(x)为1 。</p>
<p>注：信息熵的定义<br>$$<br>H(x) = -\displaystyle\sum_ip_i\log(p_i)<br>$$</p>
<p>交叉熵误差的值只由正确解标签对应的输出结果决定。</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉熵误差：计算正确解标签对应输出的自然对数，正确解对应输出值越大(即概率越接近1)越接近0  [标签是one-hot编码]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span>(<span class="params">y, t</span>):</span></span><br><span class="line">    delt = <span class="number">1e-7</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + delt)) <span class="comment"># 加上 delt 是为了防止 log(0) 的出现导致后序计算无法继续</span></span><br></pre></td></tr></table></figure>

<h2 id="mini-batch学习"><a href="#mini-batch学习" class="headerlink" title="mini-batch学习"></a>mini-batch学习</h2><p>使用训练数据进行学习，严格来说就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。</p>
<p>单个数据的损失函数值不具有代表性，全部数据的损失函数之求和再取平均值计算量又过大，所以这里取折中的方法，从全部数据中选出一批数据（称为<code>mini-batch</code>，小批量），然后对每个<code>mini-batch</code>学习。</p>
<blockquote>
<p>这里引入<code>epoch</code>的概念，一个<code>epoch</code>表示学习中所有训练数据均被使用过一次时的迭代次数。</p>
<p>若总数据量为1000，mini-batch为100，则每个epoch是10个mini-batch</p>
</blockquote>
<p>mini-batch交叉熵误差实现实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以处理单个数据和批量数据的交叉熵  [标签是one-hot编码]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span>(<span class="params">y, t</span>):</span></span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + <span class="number">1e-7</span>)) / batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在标签为非 one-hot 编码时交叉熵的计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span>(<span class="params">y, t</span>):</span></span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(np.log(y[np.arange(batch_size), t] + <span class="number">1e-7</span>)) / batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通用化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span>(<span class="params">y, t</span>):</span></span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果标签是one-hot编码, 同过计算最大索引来寻找真正的标签</span></span><br><span class="line">    <span class="keyword">if</span> t.size == y.size: </span><br><span class="line">        t = t.argmax(axis = <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># y的第一个维度为批量大小</span></span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(np.log(y[np.arange(batch_size), t] + <span class="number">1e-7</span>)) / batch_size</span><br></pre></td></tr></table></figure>



<h2 id="导入损失函数的原因"><a href="#导入损失函数的原因" class="headerlink" title="导入损失函数的原因"></a>导入损失函数的原因</h2><blockquote>
<p>​    在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则权重参数的导数在绝大多数地方都会变为0，因为权重的微小改变并不会导致识别精度的连续的变化。  </p>
<p>​    识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续、突然的变化。作为激活函数的<em>阶跃函数</em>也有同样的情况，因为其导数在绝大多数地方都为0 。</p>
</blockquote>
<h2 id="梯度，用数值微分法求梯度"><a href="#梯度，用数值微分法求梯度" class="headerlink" title="梯度，用数值微分法求梯度"></a>梯度，用数值微分法求梯度</h2><h3 id="梯度的概念"><a href="#梯度的概念" class="headerlink" title="梯度的概念"></a>梯度的概念</h3><blockquote>
<p>由<em>全部变量</em>的<em>偏导数</em>汇总而成的<em>向量</em>称为<strong>梯度</strong>。</p>
</blockquote>
<h3 id="为何这里引入梯度"><a href="#为何这里引入梯度" class="headerlink" title="为何这里引入梯度"></a>为何这里引入梯度</h3><blockquote>
<p>梯度会指向各点处的函数值降低的方向，严格来说，梯度指向的方向是各点处的函数值减小最多的方向。利用这一性质可以来确定减小损失函数值的方法。</p>
</blockquote>
<h3 id="利用数值微分来求梯度"><a href="#利用数值微分来求梯度" class="headerlink" title="利用数值微分来求梯度"></a>利用数值微分来求梯度</h3><h4 id="数值微分的概念"><a href="#数值微分的概念" class="headerlink" title="数值微分的概念"></a>数值微分的概念</h4><blockquote>
<p>利用微小的差分求导数的过程称为数值微分。</p>
</blockquote>
<h4 id="求梯度"><a href="#求梯度" class="headerlink" title="求梯度"></a>求梯度</h4><p>代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##  利用数值微分来计算梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numerical_gradient</span>(<span class="params">f, x</span>):</span></span><br><span class="line">    <span class="comment"># x 实际上指的是权重</span></span><br><span class="line">    h = <span class="number">1e-4</span></span><br><span class="line">    grad = np.zeros_like(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 依次计算每个变量的偏导数，最后汇总成一个向量</span></span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(x.size):</span><br><span class="line">        tmp_val = x[idx]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 f(x + h)</span></span><br><span class="line">        x[idx] = tmp_val + h</span><br><span class="line">        fxh1 = f(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 f(x - h)</span></span><br><span class="line">        x[idx] = tmp_val - h </span><br><span class="line">        fxh2 = f(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将计算出的单个变量的偏导数加入向量组</span></span><br><span class="line">        grad[idx] = (fxh1 - fxh2) / (<span class="number">2</span> * h) </span><br><span class="line"></span><br><span class="line">        x[idx] = tmp_val <span class="comment"># 将变量恢复原值</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>



<h2 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h2><blockquote>
<p>​    机器学习的目的是找到最优参数，也即使损失函数值最小的参数。使用梯度来寻找损失函数最小值的方法就是梯度法。  </p>
<p>​    需要注意的是，梯度表示的是各点处的函数值减小最多的方向，却不一定是函数的最小值或真正应该前进的方向。  </p>
<p>​    梯度的方向虽然不一定指向最小值，但沿着这个方向能最大程度的减小损失函数的值。通过不断地沿着新梯度的方向前进，逐渐减小损失函数值的过程就是梯度法（这里特指梯度下降法）。</p>
</blockquote>
<p>学习率引入：</p>
<blockquote>
<p>学习率决定在一次学习中，应该学习多少，即在多大程度上根据梯度更新参数。学习率是一种超参数，超参数是由人工设定的。</p>
</blockquote>
<p>梯度下降法代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##  梯度下降法：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">f, init_x, lr = <span class="number">0.01</span>, step_num = <span class="number">100</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    f : 表示要优化的函数;</span></span><br><span class="line"><span class="string">    init_x ：x 的初始值;</span></span><br><span class="line"><span class="string">    lr: 学习率 leraning rate;</span></span><br><span class="line"><span class="string">    step_num : 是梯度法重复的次数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = init_x <span class="comment"># init_x 即初始值，也即初始权重</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step_num): <span class="comment"># 共更新 step_num 次，根据梯度反复更新参数来学习</span></span><br><span class="line">        grad = numerical_gradient(f, x) <span class="comment"># 计算梯度</span></span><br><span class="line">        x -= lr * grad <span class="comment"># 向减小的方向更新， 以得到极小值</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="神经网络的学习步骤"><a href="#神经网络的学习步骤" class="headerlink" title="神经网络的学习步骤"></a>神经网络的学习步骤</h2><ul>
<li><p>前提：</p>
<blockquote>
<p>神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分为以下四步：</p>
</blockquote>
<ul>
<li><p>步骤1——mini-batch</p>
<p>从训练数据中随机选出一部分数据，这部分数据称为mini-batch。目标是减小mini-batch的损失函数的值。</p>
</li>
<li><p>步骤2——计算梯度</p>
<p>求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向。</p>
</li>
<li><p>步骤3——更新参数</p>
<p>将权重参数沿梯度方向进行<strong>微小更新</strong>。</p>
</li>
<li><p>步骤4——重复</p>
<p>重复步骤1、步骤2、步骤3</p>
</li>
</ul>
</li>
</ul>
<p>​    </p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
		data-enable='true'
        data-ae='false'
        data-ci=''
        data-cs=''
        data-r=''
        data-o=''
        data-a=''
        data-d='false'
    >查看评论</div>


    </div>
    
        <div class='side'>
			<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-number">1.</span> <span class="toc-text">学习的含义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">2.1.</span> <span class="toc-text">均方误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE-cross-entropy"><span class="toc-number">2.2.</span> <span class="toc-text">交叉熵误差  cross entropy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mini-batch%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.</span> <span class="toc-text">mini-batch学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="toc-number">4.</span> <span class="toc-text">导入损失函数的原因</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%EF%BC%8C%E7%94%A8%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86%E6%B3%95%E6%B1%82%E6%A2%AF%E5%BA%A6"><span class="toc-number">5.</span> <span class="toc-text">梯度，用数值微分法求梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">5.1.</span> <span class="toc-text">梯度的概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BD%95%E8%BF%99%E9%87%8C%E5%BC%95%E5%85%A5%E6%A2%AF%E5%BA%A6"><span class="toc-number">5.2.</span> <span class="toc-text">为何这里引入梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86%E6%9D%A5%E6%B1%82%E6%A2%AF%E5%BA%A6"><span class="toc-number">5.3.</span> <span class="toc-text">利用数值微分来求梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">5.3.1.</span> <span class="toc-text">数值微分的概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%82%E6%A2%AF%E5%BA%A6"><span class="toc-number">5.3.2.</span> <span class="toc-text">求梯度</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="toc-number">6.</span> <span class="toc-text">梯度法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%AD%A5%E9%AA%A4"><span class="toc-number">7.</span> <span class="toc-text">神经网络的学习步骤</span></a></li></ol>	
        </div>
    
</div>


    </div>
</div>
</body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</html>
