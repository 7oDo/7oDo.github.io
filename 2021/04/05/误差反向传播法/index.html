
<!DOCTYPE html>
<html lang="zh-CN" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>0x04 误差反向传播法 - 如是</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="Fechin,"> 
    <meta name="description" content="计算图计算图将计算过程用图形表示出来，这里的图形指的是数据结构图，通过多个节点和边表示。如下图所示：

为何使用计算图？计算图集中精力于局部计算，无论全局的计算有多复杂，各个步骤所要做的就是对象节点,"> 
    <meta name="author" content="如是"> 
    <link rel="alternative" href="atom.xml" title="如是" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.jpg"> 
    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

    
<link rel="stylesheet" href="/css/diaspora.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>

<body class="loading">
    <span id="config-title" style="display:none">如是</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="iconfont icon-home image-icon" href="javascript:;" data-url="http://example.com"></a>
    <div title="播放/暂停" class="iconfont icon-play"></div>
    <h3 class="subtitle">0x04 误差反向传播法</h3>
    <div class="social">
        <div>
            <div class="share">
                <a title="获取二维码" class="iconfont icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">0x04 误差反向传播法</h1>
        <div class="stuff">
            <span>四月 05, 2021</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/DL/" rel="tag">DL</a></li></ul>


        </div>
        <div class="content markdown">
            <h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p>计算图将计算过程用图形表示出来，这里的图形指的是数据结构图，通过多个节点和边表示。如下图所示：<span id="more"></span></p>
<p><img src="https://s2.ax1x.com/2019/07/22/eCsVxg.png" alt="image"></p>
<h4 id="为何使用计算图？"><a href="#为何使用计算图？" class="headerlink" title="为何使用计算图？"></a>为何使用计算图？</h4><p>计算图集中精力于<strong>局部计算</strong>，无论全局的计算有多复杂，各个步骤所要做的就是对象节点的局部计算，<strong>简化了问题</strong>。其次，通过局部计算可以<strong>将中间的计算结果全部保存</strong>起来，并通过<strong>反向传播高效计算导数</strong>进而<strong>计算梯度</strong>。</p>
<p>反向传播中的局部计算如下图所示：</p>
<p><img src="https://s2.ax1x.com/2019/07/22/eCsBi6.png" alt="image"></p>
<h4 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h4><p>链式法则是关于复合函数的导数的性质：</p>
<blockquote>
<p>如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。</p>
</blockquote>
<p>$$<br>\frac{\partial z}{\partial x} = \frac{\partial z}{\partial t}\frac{\partial t}{\partial x}<br>$$</p>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>基于链式法则，反向传播的计算顺序是，将节点的输入信号乘以节点的局部导数（偏导数），然后再传递给下一个节点。</p>
<p>利用链式法则：<br>$$<br>\frac{\partial e}{\partial a} = \frac{\partial e}{\partial c}\frac{\partial c}{\partial a}<br>$$</p>
<h5 id="加法节点的反向传播"><a href="#加法节点的反向传播" class="headerlink" title="加法节点的反向传播"></a>加法节点的反向传播</h5><p>基于加法的导数为1，所以<strong>加法节点的反向传播将上游的值原封不动的输出到下游</strong>。</p>
<h5 id="乘法节点的反向传播"><a href="#乘法节点的反向传播" class="headerlink" title="乘法节点的反向传播"></a>乘法节点的反向传播</h5><p>$$<br>z = xy \\<br>\frac{\partial z}{\partial x} = y  \\<br>\frac{\partial z}{\partial y} = x<br>$$</p>
<p>乘法节点的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。“翻转值”表示的是一种关系，在上式中x的翻转值是y，y的翻转值是x 。</p>
<h4 id="简单层的实现"><a href="#简单层的实现" class="headerlink" title="简单层的实现"></a>简单层的实现</h4><p>层的实现中有两个共同的方法（接口）<code>forward()</code>和<code>backward()</code>，分别对应正向传播和反向传播。</p>
<h5 id="乘法层的实现"><a href="#乘法层的实现" class="headerlink" title="乘法层的实现"></a>乘法层的实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 乘法层的实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MulLayer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        乘法层的实现;</span></span><br><span class="line"><span class="string">        初始化实例变量，保存正向传播时的输入值;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line">        self.y = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        正向传播, 将 x 于 y 相乘</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        out = x* y</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backword</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        反向传播, 将上游传来的导数 dout 与 对应的翻转值相乘然后返回</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dx = dout * self.y <span class="comment"># 上游值 乘 翻转值</span></span><br><span class="line">        dy = dout * self.x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br></pre></td></tr></table></figure>

<h5 id="加法层的实现"><a href="#加法层的实现" class="headerlink" title="加法层的实现"></a>加法层的实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法层的实现</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddLayer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        正向传播</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = x + y</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backword</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;反向传播&quot;&quot;&quot;</span></span><br><span class="line">        dx = dout * <span class="number">1</span></span><br><span class="line">        dy = dout * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> dx, dy</span><br></pre></td></tr></table></figure>



<h4 id="激活函数层的实现"><a href="#激活函数层的实现" class="headerlink" title="激活函数层的实现"></a>激活函数层的实现</h4><h5 id="ReLU层的实现"><a href="#ReLU层的实现" class="headerlink" title="ReLU层的实现"></a><code>ReLU</code>层的实现</h5><p>$$<br>y = \begin{cases}x&amp;(x&gt;0)\\ 0&amp;(x\leq0) \end{cases}<br>$$</p>
<p>$$<br>\frac{\partial y}{\partial x} = \begin{cases}1&amp;(x&gt;0) \\ 0&amp;(x\leq0)\end{cases}<br>$$<br>如果正向传播时的输入大于0，则反向传播会将上游的值原封不懂得传给下游。反过来，如果正向传播时的输入小于或等于0，则反向传播中传给下游的信号将会停在此处。</p>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## ReLU 层的实现</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">ReLU(修正线性单元) 层，即使用了ReLU作为激活函数的层（当x大于0，原封不动向前传播[导数为1]，当x小于0，向前传播0[此时数值为常数，导数为0]）；</span></span><br><span class="line"><span class="string">因此，如果正向传播的x大于0，则反向传播会将上游的导数值乘1传给下游，即原封不动的传给下游。反过来，如果正向传播的x小于0，则反向传播中传给下游的信号将停在此处；</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReLU</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.mask = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.mask = (x &lt;= <span class="number">0</span>) <span class="comment"># 标记正向传播时小于等于0的数值的位置</span></span><br><span class="line">        out = x.copy()</span><br><span class="line">        out[self.mask] = <span class="number">0</span> <span class="comment"># 将小于等于0的置为0</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        dout[self.mask] = <span class="number">0</span> <span class="comment"># 将正向传播时小于等于0的置零, 大于0的原封不动的传给下游</span></span><br><span class="line">        dx = dout</span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>

<h5 id="sigmoid层的实现"><a href="#sigmoid层的实现" class="headerlink" title="sigmoid层的实现"></a><code>sigmoid</code>层的实现</h5><p>$$<br>y = \frac{1}{1+e^{-x}}<br>$$</p>
<p>$$<br>\frac{\partial y}{\partial x} = -\frac{e^{-x}}{(1+e^{-x})^2} = y(1-y)<br>$$<br>实现代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## sigmoid 激活函数层的实现</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Sigmoid 层， 即使用了sigmoid作为激活函数的层，输入为x输出为 y = 1/(1+exp(-x))</span></span><br><span class="line"><span class="string">根据计算图可得知，反向传播时输入为L, 输出为 (dL/dy) * (y) * (1-y)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        slef.out = <span class="literal">None</span> <span class="comment"># 用于保存正向传播时的输出 y</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">        self.out = out <span class="comment"># 保存正向传播时输出, 即公式中的 y</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        dx = dout * (self.out) * (<span class="number">1</span> - self.out) <span class="comment"># dout 即dL/dy 反向传播时的输入</span></span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>

<h4 id="Affine-和-Softmax层的实现"><a href="#Affine-和-Softmax层的实现" class="headerlink" title="Affine 和 Softmax层的实现"></a><code>Affine</code> 和 <code>Softmax</code>层的实现</h4><h5 id="Affine层"><a href="#Affine层" class="headerlink" title="Affine层"></a><code>Affine</code>层</h5><blockquote>
<p>神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”（仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算和加偏置运算）。因此，这里将进行仿射变换的处理实现为“Affine”层。</p>
</blockquote>
<p>代码实现实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Affine 仿射层的实现(全连接层)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">仿射层：在神经网络的正向传播中，输入会与权重做矩阵乘法运算然后加上偏置，</span></span><br><span class="line"><span class="string">在几何学中被称为仿射变换(仿射变换包括一次线性变换和一次平移，在这里分别对应与权重矩阵做乘法运算和加上偏置)，对应公式 y = X dot W + b</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">在反向传播时，经过计算图的推导:</span></span><br><span class="line"><span class="string">dL/dx = dL/dy dot W.T</span></span><br><span class="line"><span class="string">dL/dW = X.T dot dL/dy</span></span><br><span class="line"><span class="string">dL/db = dL/dy</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Affine</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, W, b</span>):</span></span><br><span class="line">        self.W = W</span><br><span class="line">        self.b = b</span><br><span class="line">        self.x = <span class="literal">None</span></span><br><span class="line">        self.dw = <span class="literal">None</span></span><br><span class="line">        self.db = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        self.x = x</span><br><span class="line">        out = np.dot(x, self.W) + self.b</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout</span>):</span></span><br><span class="line">        <span class="comment"># dout 即 dL/dy  即上层的传给下层的值，即反向输入</span></span><br><span class="line">        dx = np.dot(dout, self.W.T)</span><br><span class="line">        self.dw = np.dot(self.x.T, dout)</span><br><span class="line">        self.db = np.<span class="built_in">sum</span>(dout, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>

<h5 id="Softmax-with-Loss层实现"><a href="#Softmax-with-Loss层实现" class="headerlink" title="Softmax-with-Loss层实现"></a><code>Softmax-with-Loss</code>层实现</h5><p>代码实现实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Softmax-with-Loss 层的实现</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">输出层使用softmax函数进行正规化，将输出值放缩到(0,1)，这里再使用交叉熵误差作为损失函数，因此叫做 softmax-with-loss 层，使用交叉熵误差时能将输出与标签的差 y-t 作为反向传播时的输入，实际上正是为了是y-t作为方向传播时的输入才使用了设计了交叉熵误差作为损失函数</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxWithLoss</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.loss = <span class="literal">None</span> <span class="comment"># 损失</span></span><br><span class="line">        self.y = <span class="literal">None</span>  <span class="comment"># 输出</span></span><br><span class="line">        self.t = <span class="literal">None</span>  <span class="comment"># 标签</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, t</span>):</span></span><br><span class="line">        self.t = t</span><br><span class="line">        self.y = softmax(t) <span class="comment"># 将输入正规化放缩作为输出</span></span><br><span class="line">        self.loss = cross_entropy_error(y, t) <span class="comment"># 使用交叉熵误差作为损失函数计算损失</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self, dout = <span class="number">1</span></span>):</span></span><br><span class="line">        batch_size = t.shape[<span class="number">0</span>]</span><br><span class="line">        dx = (self.t - self.y) / batch_size <span class="comment"># 计算反向传播时传递给下游的输入， 单个数据的误差</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>



<h4 id="用误差反向传播法求梯度"><a href="#用误差反向传播法求梯度" class="headerlink" title="用误差反向传播法求梯度"></a>用误差反向传播法求梯度</h4><p>实现代码实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过误差反向传播法计算损失函数关于权重参数的梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">self, x, t</span>):</span></span><br><span class="line">    <span class="comment"># forward 正向传播的输出, 需要注意的时这里执行了一次预测才得到了损失，所以已经将数据输入到了网络中，网络的参数已经保留	   了数据</span></span><br><span class="line">    self.loss(x, t)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># backward 反向传播时的输入</span></span><br><span class="line">    dout =<span class="number">1</span> </span><br><span class="line">    dout = self.lastlayer.backward(dout)<span class="comment"># 反向传到最后一个输出层</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 建立所有输入层和隐藏的层列表, 并将其逆向</span></span><br><span class="line">    layers = <span class="built_in">list</span>(self.layers.values())</span><br><span class="line">    layers.reverse()</span><br><span class="line">    <span class="comment"># 反向传播，依次往下游传播</span></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> layers:</span><br><span class="line">        dout = layer.backward(dout)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取反向传播时的到的各权重参数的梯度</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    grads[<span class="string">&#x27;W1&#x27;</span>] = self.layers[<span class="string">&#x27;Affine1&#x27;</span>].dW</span><br><span class="line">    grads[<span class="string">&#x27;b1&#x27;</span>] = self.layers[<span class="string">&#x27;Affine1&#x27;</span>].db</span><br><span class="line">    grads[<span class="string">&#x27;W2&#x27;</span>] = self.layers[<span class="string">&#x27;Affine2&#x27;</span>].dW</span><br><span class="line">    grads[<span class="string">&#x27;b2&#x27;</span>] = self.layers[<span class="string">&#x27;Affine2&#x27;</span>].db</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line">    </span><br></pre></td></tr></table></figure>




            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='http://link.hhtjim.com/163/425570952.mp3'></li>
                        
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
		data-enable='true'
        data-ae='false'
        data-ci=''
        data-cs=''
        data-r=''
        data-o=''
        data-a=''
        data-d='false'
    >查看评论</div>


    </div>
    
        <div class='side'>
			<ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">1.</span> <span class="toc-text">计算图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BD%95%E4%BD%BF%E7%94%A8%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">为何使用计算图？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-number">3.</span> <span class="toc-text">链式法则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8A%A0%E6%B3%95%E8%8A%82%E7%82%B9%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.1.</span> <span class="toc-text">加法节点的反向传播</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B9%98%E6%B3%95%E8%8A%82%E7%82%B9%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">4.2.</span> <span class="toc-text">乘法节点的反向传播</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.</span> <span class="toc-text">简单层的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B9%98%E6%B3%95%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.1.</span> <span class="toc-text">乘法层的实现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8A%A0%E6%B3%95%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.2.</span> <span class="toc-text">加法层的实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.</span> <span class="toc-text">激活函数层的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#ReLU%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.1.</span> <span class="toc-text">ReLU层的实现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sigmoid%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.2.</span> <span class="toc-text">sigmoid层的实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Affine-%E5%92%8C-Softmax%E5%B1%82%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.</span> <span class="toc-text">Affine 和 Softmax层的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Affine%E5%B1%82"><span class="toc-number">7.1.</span> <span class="toc-text">Affine层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Softmax-with-Loss%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.2.</span> <span class="toc-text">Softmax-with-Loss层实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E6%B1%82%E6%A2%AF%E5%BA%A6"><span class="toc-number">8.</span> <span class="toc-text">用误差反向传播法求梯度</span></a></li></ol>	
        </div>
    
</div>


    </div>
</div>
</body>

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>


<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/typed.js"></script>
<script src="/js/diaspora.js"></script>


<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">


<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>




</html>
